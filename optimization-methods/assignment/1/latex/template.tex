\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

% Required package
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{matlab-prettifier}
\usepackage{float}
\usepackage[export]{adjustbox}
\usepackage{multirow}

\renewcommand{\thesubsection}{\arabic{subsection}}

% vector shortcut
\newcommand{\myvec}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\myex}[1]{\begin{equation*}\begin{aligned} #1 \end{aligned}\end{equation*}}


\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{Tuesday, 19 March 2024, 12:00 AM}

\serieheader
{Optimization Methods}
{2024}
{\textbf{Student:} Jeferson Morales Mariciano \\\\}
{\textbf{Discussed with:} }
{Assignment 1}{}
\newline

%\assignmentpolicy


% EXERCISE 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1}
Vector-valued function $f : \mathbb{R}^2 \rightarrow \mathbb{R}$

\begin{equation}
    f(x_1, x_2) = 200 (x_2 - x_1^2)^2 + (1 - x_1)^2
\end{equation}

\subsection{}
Compute gradient $\nabla f : \mathbb{R}^2 \rightarrow \mathbb{R}^2$
and Hessian $H_f : \mathbb{R}^2 \rightarrow \mathbb{R}^{2 \times 2}$. \newline

\begin{equation*}
\begin{aligned}
    \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
     & \mapsto 200 (x_2 - x_1^2) (x_2 - x_1^2) + (1 - x_1) (1 - x_1) \\
     & = 200 (x_2^2 - 2x_2 x_1^2 + x_1^4) + (1 - 2x_1 + x_1^2)       \\
     & = 200 x_2^2 - 400 x_2 x_1^2 + 200 x_1^4 + 1 - 2x_1 + x_1^2    \\
    \nabla f
     & = \begin{bmatrix}
             \frac{\partial f}{\partial x_1} \\
             \frac{\partial f}{\partial x_2}
         \end{bmatrix}
    = \begin{bmatrix}
          -800 x_2 x_1 + 800 x_1^3 + 2x_1 - 2 \\
          400 x_2 - 400 x_1^2
      \end{bmatrix} \\
    H_f
     & =\begin{bmatrix}
            \frac{\partial^2 f}{\partial x_1^2}            
            & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\
            \frac{\partial^2 f}{\partial x_2 \partial x_1} 
            & \frac{\partial^2 f}{\partial x_2^2}            \\
        \end{bmatrix}
    = \begin{bmatrix}
          -800 x_2 + 2400 x_1^2 + 2 & - 800 x_1 \\
          -800 x_1                  & 400
      \end{bmatrix}
\end{aligned}
\end{equation*}

\subsection{}
Write Taylor's expansion of $f$ up to 2nd order around point
$(x_1, x_2) = (0, 0)$. \newline

Given $x_0 = (0, 0)$, $f(x_0) = 1$, 
$\nabla f(x_0) = \begin{bmatrix} -2 \\ 0 \end{bmatrix}$, 
$H_f(x_0) = \begin{bmatrix} 2 & 0 \\ 0 & 400 \end{bmatrix}$, 
incognites $h = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$

\begin{equation*}
\begin{aligned}
    f(x_0 + h)
     & = f(x_0) + h^T \nabla f(x_0) + \frac{1}{2} h^T H_f(x_0) h + o(|| h ||^2)                                                                                                                                                                                                                                                                                      \\
     & = 1 + \myvec{x_1 & x_2} \myvec{-2 \\ 0 } + \frac{1}{2} \myvec{x_1 & x_2} \myvec{2 & 0 \\ 0 & 400} \myvec{x_1 \\ x_2} + o(|| h ||^2) \\
     & = 1 - 2x_1 + \frac{1}{2} \myvec{2x_1 & 400x_2} \myvec{x_1 \\ x_2} + o(|| h ||^2) \\ 
     & = 1 - 2x_1 + x_1^2 + 200x_2^2 + o(|| h ||^2) \approx f(x_1, x_2)
\end{aligned}
\end{equation*}

% EXERCISE 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 2}
Quadratic minimization problem with $A \in \mathbb{R}^{n \times n}$ is SPD 
$\land \, x,b \in \mathbb{R}^n$.

\begin{equation}
    \min_{x \in \mathbb{R}^n} J(x) = \frac{1}{2} x^T A x - b^T x
\end{equation}

\subsection{}
Compute gradient and Hessian of $J$. \newline

\myex{
    \nabla J(x) = A \myvec{x_1 \\ \vdots \\ x_n} - b = Ax - b
}

\myex{
    H_J(x) = A
}

\subsection{}
Write down the 1st Order Necessary Conditions. \newline

If $x^*$ is local minimizer 
$\land$ $f$ continuously differentiable in open neighborhood $\mathcal{N}(x^*)$ 
$\therefore$ $\nabla f(x^*) = 0$. 
$x^*$ stationary point $\because$ $\nabla f(x^*) = 0$, 
from Th 2.2 any local minimizer is a stationary point.

\myex{
    \nabla f(x^*) = 0 
    \Rightarrow \nabla J(x^*) = A x^* - b
    \Rightarrow A x^* - b = 0
    \Rightarrow A x^* = b
}

\subsection{}
Write down the 2nd Order Necessary and Sufficient Conditions. \newline

\textbf{2nd Order Necessary Conditions}:
If $x^*$ local minimizer of $f$ 
$\land$ $\exists \nabla^2 f$ continuous in open neighborhood $\mathcal{N}(x^*)$ 
$\therefore$ $\nabla f(x^*) = 0$ 
$\land$ $\nabla^2 f(x^*)$ is positive semidefinite.

\myex{
    \nabla f(x^*) = 0
    \Rightarrow \nabla J(x^*) = A x^* - b
    \Rightarrow A x^* = b
}
\myex{
    \nabla^2 f(x^*)
    \Rightarrow \nabla^2 J(x^*) = H_J = A \text{ positive semidefinite } 
    \therefore \text{ eigenvals } \lambda \geq 0 \,\, \forall \lambda \in \Lambda \,
    \land \, x^T A x \geq 0
}
\newline

\textbf{2nd Order Sufficient Conditions}:
Suppose $\nabla^2 f$ continuous in open neighborhood $\mathcal{N}(x^*)$ 
$\land$ $\nabla f(x^*) = 0$ 
$\land$ $\nabla^2 f(x^*)$ positive definite
$\therefore$ $x^*$ strict local minimizer of $f$.

\myex{
    \nabla f(x^*) = 0
    \Rightarrow \nabla J(x^*) = A x^* - b
    \Rightarrow A x^* = b
}
\myex{
    \nabla^2 f(x^*)
    \Rightarrow \nabla^2 J(x^*) = H_J = A \text{ positive definite } 
    \therefore \text{ eigenvals } \lambda > 0 \,\, \forall \lambda \in \Lambda \,
    \land \, x^T A x > 0
}

% EXERCISE 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 3}
Function

\begin{equation}
    f(x, y) = x^2 + \mu y^2
\end{equation}

\subsection{}
Write down its quadratic form. \newline

\myex{
    f(x, y) 
    & = x^2 + \mu y^2 \\
    & = \myvec{x & \mu y} \myvec{x \\ y} \\
    & = \frac{1}{2} \myvec{x & y} \myvec{2 & 0 \\ 0 & 2 \mu} \myvec{x & y} - \myvec{0 \\ 0} \vec{x} \\
    & = \frac{1}{2} \vec{x}^T A \vec{x} - \vec{b}^T \vec{x}
}

Quadratic Form $\frac{1}{2} \vec{x}^T A \vec{x} - \vec{b}^T \vec{x}$ 
with $\vec{x} = \myvec{x \\ y}$, $A \in \mathbb{R}^{2 \times 2}$, A is SPD, $\vec{b} \in \mathbb{R}^2$, $\vec{b} = \vec{0}$.

\subsection{}
Plot the surface of the functions and the corresponding contour plot for values $\mu = 1$ and $\mu = 10$. 
In both cases use the square $[-10, 10] \times [-10, 10]$.
Comment on the behaviour of the isolines.
% Matlab function: surf, contour

\subsection{}
Considering that $A$ is a symmetric positive-definite matrix, find the exact optimal step-length $\alpha$.
Show your computations.

\subsection{}
Write a Matlab code for the gradient method with maximum number of iterations $N = 100$ and a tolerance $tol = 10-8$ . 
Minimize $f$ for $\mu = (1, 10)$ and starting points: $(x0 , y0 ) = (10, 0), (0, 10), (10, 10)$.

\subsection{}
For each case plot the iterations on the energy landscape in 2D (the plot of the objective function),
the log10 of the norm of the gradient and the value of the energy function (objective function) as functions of the iterations. 
Comment the results.

\end{document}
