\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

% Required package
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{matlab-prettifier}
\usepackage{float}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsthm} % math theorems
\usepackage{ifthen}

\renewcommand{\thesubsection}{\arabic{subsection}}

% 1: command name, 2: Title, 3: subtitle, 4: label, 5: content
\newcommand{\mytheorem}[5]{\newtheorem*{#1}{#2} \begin{#1}[#3]\label{#4} #5 \end{#1}}

% 1: if numbered equation, 2: label, 3: content
\newcommand{\myex}[3]{
    \ifthenelse{\equal{#1}{true}}{
        \begin{equation} \label{#2} \begin{aligned} #3 \end{aligned} \end{equation}
    }{
        \begin{equation*} \label{#2} \begin{aligned} #3 \end{aligned} \end{equation*}
    }
}

% vector shortcut
\newcommand{\myvec}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\myFigureEnergy}[3]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=\paperwidth, trim={9cm 0cm -2cm 0cm}]{./figures/#3}
    \end{figure}
}
\newcommand{\myFigureComparison}[4]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=.2\paperwidth, trim={8.5cm 0cm 0.5cm 0cm}]{./figures/#3}
    \includegraphics[width=.2\paperwidth, trim={0.5cm 0cm 8.5cm 0cm}]{./figures/#4}
    \end{figure}
}

\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{Sunday, 12 May 2024, 11:59 PM}

\serieheader
{Optimization Methods}
{2024}
{\textbf{Student:} Jeferson Morales Mariciano \\\\}
{\textbf{Discussed with:} Leonardo Birindelli}
{Midterm}{}
\newline

%\assignmentpolicy


% EXERCISE 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 1}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ be given by
$f = \frac{1}{2} x^T Ax - b^T x$
with $A$ symmetric positive definite.
Let $x_m$ be the minimizer of the function $f$.
Let $v$ be an eigenvector of $A$, and let $\lambda$ be the associated eigenvalue.
Suppose that we use Stepeest Descent (SD) method to minimize $f$
and the starting point for the SD algorithm is $x_0 = x_m + v$.

\subsection*{1.}
Prove that the gradient at $x_0$ is $\nabla f(x_0) = \lambda v$.

\subsection*{2.}
How many iterations does the SD method take to minimize the function $f$
if we use the optimal step length?
Show the computations behind your reasoning.\\


Matlab scripts are provided in \textit{/code} folder.
The 2 main files to run are: \textit{GD.m, Newton.m}.
They handle both computations and visualization of the Rosenbrock's function with the corresponding methods.

\clearpage

% EXERCISE 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 2}
Given a starting point $x_0 \in \mathbb{R}^n$
and a set of conjugate directions $\{ p_0, p_1, \ldots, p_{n-1} \}$,
we generate the sequence $\{ x_k \}$ by setting
\myex{true}{eq:xk}{
    x_{k+1} = x_k + \alpha_k p_k
}
where
\myex{true}{eq:alphak}{
    \alpha_k = - \frac{r_k^T p_k}{p_k^T A p_k}
}
and $r_k$ is the residual, as defined in class.
Consider the following theorem:
\mytheorem{theorem}{Theorem 1}{}{theorem:ex-2}{
    For any $x_0 \in \mathbb{R}^n$ the sequence $\{ x_k \}$
    generated by the conjugate gradient direction algorithm
    (\ref{eq:xk}) and (\ref{eq:alphak}),
    converges to the solution $x^*$ of the linear system $Ax = b$ in at most $n$ steps.
}

\noindent Prove the theorem and explain casrefully every step of your reasoning.\\


Matlab scripts are provided in \textit{/code} folder.
The main file to run is \textit{BFGS.m}.
It handles both computations and visualization of the Rosenbrock's function with the corresponding method.

% \begin{table*}[h]
%     \centering
%     \caption{Comparison of Iterations for Rosenbrock's Function}
%     \label{tab:ex2-iterations-comparison}
%     \begin{tabular}{@{}ccc@{}}
%         \toprule
%         Newton & BFGS & Steepest Descent \\
%         \midrule
%         13     & 27   & 21103            \\
%         \bottomrule
%     \end{tabular}
% \end{table*}

\clearpage

% EXERCISE 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 3}

Consider the linear system $Ax = b$,
where the matrix $A$ is a symmetric positive definitive diagonal matrix
constructed in three different ways:

\begin{itemize}
    \item[ ] $A = diag([1:10])$
    \item[ ] $A = diag(ones(1,10))$
    \item[ ] $A = diag([1, 1, 1, 3, 4, 5, 5, 5, 10, 10])$
    \item[ ] $A = diag([1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0])$
\end{itemize}

\subsection*{1.}
How many distinct eigenvalues has each matrix?

\subsection*{2.}
Implement the CG method (CG.m).

\subsection*{3.}
Construct a right-hand side $b = rand(10,1)$ and apply the Conjugate Gradient method to solve the
system for each $A$.

\subsection*{4.}
Compute the logarithm energy norm of the error (i.e. $log((x - x^*)^T A(x - x^*))$)
for each matrix and plot it with respect to the number of iteration.

\subsection*{5.}
Comment on the convergence of the method for the different matrices.
What can you say observing the number of iterations obtained
and the number of clusters of the eigenvalues of the related matrix?


% EXERCISE 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 4}
Consider the Chapter 4, "Trust-region methods" of the book \textit{Numerical Optimization},
Nocedal and Wright.

\subsection*{1.}
Explain  Cauchy point method and Dogleg method, as well as the connection between them.

\subsection*{2.}
Write down the Trust-Region algorithm, along with Dogleg and Cauchy-point computations.

\subsection*{3.}
Consider the following lemma (Lemma 4.2, page 75 Numerical optimization, Nocedal and Wright)

\mytheorem{lemma}{Lemma 4.2}{}{lemma:4.2}{
    Let B be positive definite. Then,
    \begin{itemize}
        \item[(a)] $|| \tilde{p}(\tau) ||$ is an increasing function of $\tau$, and
        \item[(b)] $m(\tilde{p}(\tau))$ is a decreasing function of $\tau$
    \end{itemize}
}

\end{document}
