\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

% Required package
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{matlab-prettifier}
\usepackage{float}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsthm} % math theorems
\usepackage{ifthen}
\usepackage{physics} % responsive norm, abs, ...

\renewcommand{\thesubsection}{\arabic{subsection}}

% 1: command name, 2: Title, 3: subtitle, 4: label, 5: content
\newcommand{\mytheorem}[5]{\newtheorem*{#1}{#2} \begin{#1}[#3]\label{#4} #5 \end{#1}}

% 1: if numbered equation, 2: label, 3: content
\newcommand{\myex}[3]{
    \ifthenelse{\equal{#1}{true}}{
        \begin{equation} \label{#2} \begin{aligned} #3 \end{aligned} \end{equation}
    }{
        \begin{equation*} \label{#2} \begin{aligned} #3 \end{aligned} \end{equation*}
    }
}

% vector shortcut
\newcommand{\myvec}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\myFigureEnergy}[3]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=\paperwidth, trim={9cm 0cm -2cm 0cm}]{./figures/#3}
    \end{figure}
}
\newcommand{\myFigureComparison}[4]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=.2\paperwidth, trim={8.5cm 0cm 0.5cm 0cm}]{./figures/#3}
    \includegraphics[width=.2\paperwidth, trim={0.5cm 0cm 8.5cm 0cm}]{./figures/#4}
    \end{figure}
}

\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{Sunday, 12 May 2024, 11:59 PM}

\serieheader
{Optimization Methods}
{2024}
{\textbf{Student:} Jeferson Morales Mariciano \\\\}
{\textbf{Discussed with:} Leonardo Birindelli}
{Midterm}{}
\newline

%\assignmentpolicy


% EXERCISE 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 1}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ be given by
$f = \frac{1}{2} x^T Ax - b^T x$
with $A$ symmetric positive definite.
Let $x_m$ be the minimizer of the function $f$.
Let $v$ be an eigenvector of $A$, and let $\lambda$ be the associated eigenvalue.
Suppose that we use Stepeest Descent (SD) method to minimize $f$
and the starting point for the SD algorithm is $x_0 = x_m + v$.

\subsection*{1.}
Prove that the gradient at $x_0$ is $\nabla f(x_0) = \lambda v$.

\subsection*{2.}
How many iterations does the SD method take to minimize the function $f$
if we use the optimal step length?
Show the computations behind your reasoning.\\


Matlab scripts are provided in \textit{/code} folder.
The 2 main files to run are: \textit{GD.m, Newton.m}.
They handle both computations and visualization of the Rosenbrock's function with the corresponding methods.

\clearpage

% EXERCISE 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 2}
Given a starting point $x_0 \in \mathbb{R}^n$
and a set of conjugate directions $\{ p_0, p_1, \ldots, p_{n-1} \}$,
we generate the sequence $\{ x_k \}$ by setting
\myex{true}{eq:xk}{
    x_{k+1} = x_k + \alpha_k p_k
}
where
\myex{true}{eq:alphak}{
    \alpha_k = - \frac{r_k^T p_k}{p_k^T A p_k}
}
and $r_k$ is the residual, as defined in class.
Consider the following theorem:
\mytheorem{theorem}{Theorem 1}{}{theorem:ex-2}{
    For any $x_0 \in \mathbb{R}^n$ the sequence $\{ x_k \}$
    generated by the conjugate gradient direction algorithm
    (\ref{eq:xk}) and (\ref{eq:alphak}),
    converges to the solution $x^*$ of the linear system $Ax = b$ in at most $n$ steps.
}

\noindent Prove the theorem and explain carefully every step of your reasoning.\\



% \begin{table*}[h]
%     \centering
%     \caption{Comparison of Iterations for Rosenbrock's Function}
%     \label{tab:ex2-iterations-comparison}
%     \begin{tabular}{@{}ccc@{}}
%         \toprule
%         Newton & BFGS & Steepest Descent \\
%         \midrule
%         13     & 27   & 21103            \\
%         \bottomrule
%     \end{tabular}
% \end{table*}

\clearpage

% EXERCISE 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 3}

Consider the linear system $Ax = b$,
where the matrix $A$ is a symmetric positive definitive diagonal matrix
constructed in three different ways:

\begin{itemize}
    \item[ ] $A = diag([1:10])$
    \item[ ] $A = diag(ones(1,10))$
    \item[ ] $A = diag([1, 1, 1, 3, 4, 5, 5, 5, 10, 10])$
    \item[ ] $A = diag([1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0])$
\end{itemize}

\subsection*{1.}
How many distinct eigenvalues has each matrix?

\subsection*{2.}
Implement the CG method (CG.m).

Matlab scripts are provided in \textit{/code} folder.
The main file to run is \textit{CG.m}.
It handles both computations and visualization of the Rosenbrock's function.

\subsection*{3.}
Construct a right-hand side $b = rand(10,1)$ and apply the Conjugate Gradient method to solve the
system for each $A$.

\subsection*{4.}
Compute the logarithm energy norm of the error (i.e. $log((x - x^*)^T A(x - x^*))$)
for each matrix and plot it with respect to the number of iteration.

\subsection*{5.}
Comment on the convergence of the method for the different matrices.
What can you say observing the number of iterations obtained
and the number of clusters of the eigenvalues of the related matrix?


% EXERCISE 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 4}
Consider the Chapter 4, "Trust-region methods" of the book \textit{Numerical Optimization},
Nocedal and Wright.

\subsection*{1.}
Explain  Cauchy point method and Dogleg method, as well as the connection between them.

\subsection*{2.}
Write down the Trust-Region algorithm, along with Dogleg and Cauchy-point computations.

\subsection*{3.}
Consider the following lemma (Lemma 4.2, page 75 Numerical optimization, Nocedal and Wright)

\mytheorem{lemma}{Lemma 4.2}{}{lemma:4.2}{
    Let B be positive definite. Then,
    \begin{itemize}
        \item[(a)] $|| \tilde{p}(\tau) ||$ is an increasing function of $\tau$, and
        \item[(b)] $m(\tilde{p}(\tau))$ is a decreasing function of $\tau$
    \end{itemize}
}

Read carefully the proof and explain in detail how each step is obtained.\\

\noindent
Recalling:

\myex{false}{eq:dogleg-path}{
    \tilde{p}(\tau) = \begin{cases}
        \tau p^U                                              & 0 \leq \tau \leq 1 \\
        p^U + \left( \tau - 1 \right) \left(p^B - p^U \right) & 1 \leq \tau \leq 2 \\
    \end{cases}
}

\noindent First, let's prove that $\norm{\tilde{p}(\tau)}$
is an increasing function with. $\tau \in [0, 1]$.

\begin{proof}
    Let's define $\phi(\tau) = \norm{\tilde{p}(\tau)}$.
    By computing $\frac{\partial \phi}{\partial \tau}$,
    since $\norm{p^U}$ is a positive constant,
    the derivative is always positive and the function is monotonically increasing.

    \myex{false}{eq:ex4-3-1}{
        \phi(\tau) &= \norm{\tilde{p}(\tau)}
        = \abs{\tau} \cdot \norm{p^U}
        = \tau \norm{p^U} \\
        \frac{\partial \phi}{\partial \tau} &= \norm{p^U} \geq 0
    }

    thus, $\phi(\tau)$ is an increasing function with $\tau \in [0, 1]$.
\end{proof}

\noindent Second, let's prove that $m(\tilde{p}(\tau))$ is a decreasing function with
$\tau \in [0, 1]$.

\begin{proof}
    Let's expand the function $m(\tilde{p}(\tau))$.
    After computing its $\frac{\partial m}{\partial \tau}$,
    we expand $p^U$ by its definition and simplify the expression.
    Remember $B$ is SPD.
    At the end, since $\tau \in [0, 1]$, the expression $(\tau - 1) \leq 0$,
    thus the function is monotonically decreasing.

    \myex{false}{eq:ex4-3-2}{
        m(\tilde{p}(\tau))
        &= f + g^T \tilde{p}(\tau) + \frac{1}{2} \tilde{p}(\tau)^T B \tilde{p}(\tau) \\
        &= f + \tau g^T p^U + \frac{1}{2} \tau^2 (p^U)^T B p^U \\
        \frac{\partial m}{\partial \tau} &= g^T p^U + \tau (p^U)^T B p^U \\
        &= g^T \cdot \left( - \frac{g^T g}{g^T B g} g \right)
        + \tau \left( - \frac{g^T g}{g^T B g} g \right)^T B \left( - \frac{g^T g}{g^T B g} g \right) \\
        &= g^T \cdot \left( - \frac{\norm{g}^2}{g^T B g} g \right)
        + \tau \left( - \frac{\norm{g}^2}{g^T B g} g \right)^T B \left( - \frac{\norm{g}^2}{g^T B g} g \right) \\
        &= - \frac{\norm{g}^4}{g^T B g} +\tau \frac{\norm{g}^4}{(g^T B g)^2} g^T B g \\
        &= - \frac{\norm{g}^4}{g^T B g} +\tau \frac{\norm{g}^4}{g^T B g} \\
        &= \underbrace{\left( \tau - 1 \right)}_{\leq 0}
        \underbrace{\frac{\norm{g}^4}{g^T B g}}_{\geq 0} \leq 0 \\
    }

    thus, $m(\tilde{p}(\tau))$ is a decreasing function with $\tau \in [0, 1]$.
\end{proof}


\noindent Third, let's prove that $\norm{\tilde{p}(\tau)}$ is an increasing function with
$\tau \in [1, 2]$.

\begin{proof}{}
    Let's define $\alpha = \tau - 1$,
    and $\phi(\alpha) = \frac{1}{2} \norm{\tilde{p}(\alpha)}^2$ with $\alpha \in [0, 1]$.
    Inside $\phi(\alpha)$ we defined the squared norm $\norm{\cdot}^2$
    to delete the square root from the calculations.
    Then, simplify the expression by collecting using the square of sum which is the reason
    why we have the scalar $\frac{1}{2}$ in $\phi(\alpha)$.
    Remember $B$ is SPD.
    Afterwards, we compute $\frac{\partial \phi}{\partial \alpha}$
    and then prove $\left( p^U \right)^T \left( p^B - p^U \right) \geq 0$
    using the Cauchy-Schwarz inequality
    to show that $\phi$ is monotonically increasing.


    \myex{false}{eq:ex4-3-3}{
        \phi(\alpha) &= \frac{1}{2} \norm{\tilde{p}(\alpha)}^2 \\
        &= \frac{1}{2} \norm{p^U + \alpha \left( p^B - p^U \right)}^2 \\
        &= \frac{1}{2} \norm{p^U}^2 + \alpha \left( p^U \right)^T \left( p^B - p^U \right)
        + \frac{1}{2} \alpha^2 \norm{p^B - p^U}^2 \\
        \frac{\partial \phi}{\partial \alpha}
        &= \left( p^U \right)^T \left( p^B - p^U \right)
        + \underbrace{\alpha \norm{p^B - p^U}^2}_{\geq 0} \\
        &\geq \left( p^U \right)^T \left( p^B - p^U \right) \\
        \left( p^U \right)^T \left( p^B - p^U \right)
        &= - \left( p^U \right)^T \left( p^U - p^B \right) \\
        &=  \frac{g^T g}{g^T B g} g^T
        \cdot \left[ \left( - \frac{g^T g}{g^T B g} g \right)
            - \left( - B^{-1} g \right) \right] \\
        &= \frac{\norm{g}^2}{g^T B g} g^T
        \cdot  \left( B^{-1} g
        - \frac{\norm{g}^2}{g^T B g} g \right) \\
        &= \frac{\norm{g}^2}{g^T B g} g^T B^{-1} g
        \cdot  \left( 1 - \frac{\norm{g}^2}{g^T B g B^{-1} g} g \right) \\
        &= \underbrace{\frac{\norm{g}^2}{g^T B g} g^T B^{-1} g}_{\geq 0}
        \cdot \left( 1 - \frac{\norm{g}^4}{
            \left( g^T B g \right) \left( g^T B^{-1} g \right)
        } \right) \\
        \text{let } u = B^{-1} g, v = B g
        &\Rightarrow \abs{u^T v}^2 \leq \norm{u}^2 \cdot \norm{v}^2 \\
        &\Rightarrow \abs{g^T B^{-1} B g}^2 \leq \norm{B^{-1} g}^2 \cdot \norm{B g}^2 \\
        &\Rightarrow \norm{g}^4 \leq \left( g^T B^{-1} g \right) \left( g^T B g \right) \\
        &\Rightarrow \frac{\norm{g}^4}{
            \left( g^T B g \right) \left( g^T B^{-1} g \right)
        } \leq 1 \\
        &\Rightarrow 1 - \frac{\norm{g}^4}{
            \left( g^T B g \right) \left( g^T B^{-1} g \right)
        } \geq 0
    }

\end{proof}

\noindent Lastly, let's prove that $m(\tilde{p}(\tau))$ is a decreasing function with
$\tau \in [1, 2]$.

\begin{proof}
    the proof
\end{proof}

\end{document}
