\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

% Required package
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{matlab-prettifier}
\usepackage{float}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsthm} % math theorems
\usepackage{ifthen}
\usepackage{physics} % responsive norm, abs, ...

\renewcommand{\thesubsection}{\arabic{subsection}}

% 1: command name, 2: Title, 3: subtitle, 4: label, 5: content
\newcommand{\mytheorem}[5]{\newtheorem*{#1}{#2} \begin{#1}[#3]\label{#4} #5 \end{#1}}

% 1: if numbered equation, 2: label, 3: content
\newcommand{\myex}[3]{
    \ifthenelse{\equal{#1}{true}}{
        \begin{equation} \label{#2} \begin{aligned} #3 \end{aligned} \end{equation}
    }{
        \begin{equation*} \label{#2} \begin{aligned} #3 \end{aligned} \end{equation*}
    }
}

% vector shortcut
\newcommand{\myvec}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\myFigureEnergy}[3]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=\paperwidth, trim={9cm 0cm -2cm 0cm}]{./figures/#3}
    \end{figure}
}
\newcommand{\myFigureComparison}[4]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=.2\paperwidth, trim={8.5cm 0cm 0.5cm 0cm}]{./figures/#3}
    \includegraphics[width=.2\paperwidth, trim={0.5cm 0cm 8.5cm 0cm}]{./figures/#4}
    \end{figure}
}

\usepackage{tikz}
\usetikzlibrary{calc,shapes}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\newcommand{\DrawBox}[1]{%
  \begin{tikzpicture}[overlay,remember picture]
    \draw[->, shorten >=5pt, shorten <=5pt, out=70, in=130, distance=0.5cm, #1] 
    (MarkA.south) .. controls +(down:15mm) .. (MarkB.east);

    \draw[->, shorten >=5pt, shorten <=5pt, out=70, in=130, distance=0.5cm, #1] 
    (MarkA.south) .. controls +(down:17mm) .. (MarkC.east);
  \end{tikzpicture}
}


\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{Sunday, 12 May 2024, 11:59 PM}

\serieheader
{Optimization Methods}
{2024}
{\textbf{Student:} Jeferson Morales Mariciano \\\\}
{\textbf{Discussed with:} Leonardo Birindelli}
{Midterm}{}
\newline

%\assignmentpolicy


% EXERCISE 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 1}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ be given by
$f = \frac{1}{2} x^T Ax - b^T x$
with $A$ symmetric positive definite.
Let $x_m$ be the minimizer of the function $f$.
Let $v$ be an eigenvector of $A$, and let $\lambda$ be the associated eigenvalue.
Suppose that we use Stepeest Descent (SD) method to minimize $f$
and the starting point for the SD algorithm is $x_0 = x_m + v$.

\subsection*{1.}
Prove that the gradient at $x_0$ is $\nabla f(x_0) = \lambda v$. \\

\begin{proof}
    Recall that for eigenvalue $\lambda$ and eigenvector $v$ of matrix $A$,
    we have the following property $Av = \lambda v$ (1).
    Moreover, for minimizer $x_m$ of quadratic function $f$
    holds $\nabla f(x_m) = Ax_m - b = 0$ (2).
    Then:

    \myex{false}{eq:ex1-1}{
        \nabla f(x) &= Ax - b \\
        \nabla f(x_0) &= A(x_m + v) - b
        = Ax_m + Av - b
        = Ax_m - b + Av  \\
        &\overset{(2)}{=} Av \\
        &\overset{(1)}{=} \lambda v
    }
\end{proof}

% alpha opt = 1/lambda
% se ti muovi lungo autovettore, converge in un sola iterata
% riduzione del modello quadratico e' maggiore della riduzione offerta da cauchy

\subsection*{2.}
How many iterations does the SD method take to minimize the function $f$
if we use the optimal step length?
Show the computations behind your reasoning.\\

It will converge in a single iteration for quadratic function $f$, with $x_0 = x_m + v$
where $x_m$ is the function minimizer
and $v$ being an eigenvector with associated eigenvalue $\lambda$.

\begin{proof}
    Recalling the definition of optimal step length $\alpha_{opt}$ for quadratic functions:

    \myex{false}{eq:ex1-alpha-opt}{
        \alpha_{opt} = \left(
        \frac{\nabla f_k^T \nabla f_k}{\nabla f_k^T A \nabla f_k}
        \right)
    }

    and the Steepest Descent iteration defintion of subsequent points $ \left\{ x_k \right\} $:

    \myex{false}{ex1-sd-x}{
        x_{k+1} = x_k - \alpha_k \nabla f_k
    }

    We compute $x_1$:

    \myex{false}{eq:ex1-proof}{
        x_1 &= x_0 - \alpha_{opt} \nabla f_0
        = x_m + v - \frac{\nabla f_0^T \nabla f_0}{\nabla f_0^T A \nabla f_0} \nabla f_0 \\
        &\overset{1.1}{=} x_m + v - \frac{
            \left( \lambda v \right)^T \left( \lambda v \right)
        }{
            \left( \lambda v \right)^T A \left( \lambda v \right)
        } \lambda v
        = x_m + v - \frac{
            \lambda^2 v^T v
        }{
            \lambda^2 v^T A v
        } \lambda v
        = x_m + v - \frac{
            \norm{v}^2
        }{
            v^T A v
        } \lambda v \\
        &\overset{(1)}{=} x_m + v - \frac{
            \norm{v}^2
        }{
            v^T \lambda v
        } \lambda v
        = x_m + v - \frac{
            \norm{v}^2
        }{
            \lambda \norm{v}^2
        } \lambda v
        = x_m + v - \frac{1}{\lambda} \lambda v
        = x_m + v - v \\
        &= x_m
    }

    Where $x_m$ is the minimizer of the function $f$, thus the Steepest Descent method
    converges in the first single iteration exploiting the eigenvector $v$,
    hence moving along its direction,
    and using the optimal step length $\alpha_{opt}$.
\end{proof}

% EXERCISE 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 2}
Given a starting point $x_0 \in \mathbb{R}^n$
and a set of conjugate directions $\{ p_0, p_1, \ldots, p_{n-1} \}$,
we generate the sequence $\{ x_k \}$ by setting
\myex{true}{eq:ex2-xk}{
    x_{k+1} = x_k + \alpha_k p_k
}
where
\myex{true}{eq:ex2-alphak}{
    \alpha_k = - \frac{r_k^T p_k}{p_k^T A p_k}
}
and $r_k$ is the residual, as defined in class.
Consider the following theorem:
\mytheorem{theorem}{Theorem 1}{}{theorem:ex-2}{
    For any $x_0 \in \mathbb{R}^n$ the sequence $\{ x_k \}$
    generated by the conjugate gradient direction algorithm
    (\ref{eq:ex2-xk}) and (\ref{eq:ex2-alphak}),
    converges to the solution $x^*$ of the linear system $Ax = b$ in at most $n$ steps.
}

\noindent Prove the theorem and explain carefully every step of your reasoning.\\

\begin{proof}
    Since the directions $\left\{ p_i \right\}$ are linearly independent,
    they must span the whole space $\mathbb{R}^n$.
    Hence, we can write the difference between the starting point $x_0$
    and the solution $x^*$ in the following way:

    \myex{false}{eq:ex2-diff-sol-start}{
        x^* - x_0
        = \sum_{k=0}^{n-1} \sigma_k p_k
        = \sigma_0 p_0 + \sigma_1 p_1 + \ldots + \sigma_{n-1} p_{n-1}
    }

    for some choice of scalars $\sigma_k$.
    By premultiplying this expression by $p_k^T A$ and using the conjugacy property:

    \myex{true}{eq:ex2-conjugacy-prop}{
        pi^T A p_j = 0 \qquad \forall i \neq j
    }

    with computations $\forall k = 0, 1, \ldots, n-1$:

    \myex{false}{eq:ex2-diff-to-sigma}{
        x^* - x_0
        = \sigma_k p_k
        &\Rightarrow
        p_k^T A \left( x^* - x_0 \right)
        = p_k^T A \sigma_k p_k \\
        &\Rightarrow
        p_k^T A \left( x^* - x_0 \right)
        = \sigma_k p_k^T A p_k \\
        &\Rightarrow
        \frac{p_k^T A \left( x^* - x_0 \right)}{p_k^T A p_k}
        = \sigma_k
    }

    we obtain:

    \myex{true}{eq:ex2-sigma}{
        \sigma_k = \frac{p_k^T A \left( x^* - x_0 \right)}{p_k^T A p_k}
    }

    We now establish the result by showing that these coefficients $\sigma_k$
    coincide with the step lenghts $\alpha_k$ generated by Formula \ref{eq:ex2-alphak}.

    If $x_k$ is generated by Algorithm \ref{eq:ex2-xk}, \ref{eq:ex2-alphak},
    then we have:

    \myex{false}{eq:ex2-xk-proof}{
        x_{k} = x_0 + \sum_{i=0}^{k-1} \alpha_i p_i
        = x_0 + \alpha_0 p_0 + \alpha_1 p_1 + \ldots + \alpha_{k-1} p_{k-1}
    }

    By premultiplying this expression by $p_k^T A$
    and using the conjugacy property in Formula \ref{eq:ex2-conjugacy-prop},
    we have that:

    \myex{false}{eq:ex2-conjugacy-pk}{
    p_k^T A \underbrace{ \left( x_k - x_0 \right) }_{\sum_{i=0}^{k-1} \alpha_i p_i}
    = 0
    }

    and by defining the residual $r_k$ as:

    \myex{true}{eq:ex2-residual}{
        r_k = A x_k - b
    }

    and therefore:

    \myex{true}{eq:ex2-conjugacy-pkrk}{
    p_k^T A \underbrace{ \left( x^* - x_0 \right) }_{\sum_{i=0}^{n-1} \alpha_i p_i}
    &= p_k^T A \underbrace{ \left( x^* - x_k \right) }_{\sum_{i=k}^{n-1} \alpha_i p_i}
    =
    p_k^T \left( b - A x_k \right)
    \overset{(\ref{eq:ex2-residual})}{=}
    - p_k^T r_k \tikzmark{MarkA}
    }

    By comparing this relation with the $\alpha_k$ and $\sigma_k$
    in Formula \ref{eq:ex2-alphak}, \ref{eq:ex2-sigma}:

    \myex{false}{eq:ex2-comparison}{
    \alpha_k
    \overset{(\ref{eq:ex2-alphak})}{=}
    \frac{\overbrace{- r_k^T p_k}^{\tikzmark{MarkB}}}{p_k^T A p_k}
    = \frac{- p_k^T r_k}{p_k^T A p_k}
    \overset{(\ref{eq:ex2-conjugacy-pkrk})}{=}
    \frac{
    \overbrace{p_k^T A \left( x^* - x_0 \right)}^{\tikzmark{MarkC}}
    }{
    p_k^T A p_k
    }
    \overset{(\ref{eq:ex2-sigma})}{=}
    \sigma_k
    \DrawBox{black}
    }

    we conclude that $\sigma_k = \alpha_k$, giving the result to prove the theorem
    of convergence in at most $n$ steps, with $A \in \mathbb{R}^{n \times n}$ being SPD.
\end{proof}

% \begin{table*}[h]
%     \centering
%     \caption{Comparison of Iterations for Rosenbrock's Function}
%     \label{tab:ex2-iterations-comparison}
%     \begin{tabular}{@{}ccc@{}}
%         \toprule
%         Newton & BFGS & Steepest Descent \\
%         \midrule
%         13     & 27   & 21103            \\
%         \bottomrule
%     \end{tabular}
% \end{table*}

% EXERCISE 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 3}

Consider the linear system $Ax = b$,
where the matrix $A$ is a symmetric positive definitive diagonal matrix
constructed in three different ways:

\begin{itemize}
    \item[ ] $A = diag([1:10])$
    \item[ ] $A = diag(ones(1,10))$
    \item[ ] $A = diag([1, 1, 1, 3, 4, 5, 5, 5, 10, 10])$
    \item[ ] $A = diag([1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0])$
\end{itemize}

\subsection*{1.}
How many distinct eigenvalues has each matrix?\\

The following matrices has the following number of distinct eigenvalues $| \Lambda |$,
which correspond to the number of different elements in the diagonal of the matrix $A$:
\begin{itemize}
    \item[ ] $A1 = diag([1:10]) \rightarrow 10$
    \item[ ] $A2 = diag(ones(1,10)) \rightarrow 1$
    \item[ ] $A3 = diag([1, 1, 1, 3, 4, 5, 5, 5, 10, 10]) \rightarrow 5$
    \item[ ] $A4 = diag([1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]) \rightarrow 10$
\end{itemize}

\subsection*{2.}
Implement the CG method (CG.m).\\

Matlab scripts are provided in \textit{/code} folder.
Inside you will find the \textit{CG.m} implementation.

\subsection*{3.}
Construct a right-hand side $b = rand(10,1)$
and apply the Conjugate Gradient method to solve the system for each $A$.\\

The main file to run is \textit{ex3.m}, which uses the \textit{CG.m} implementation.
It handles the computation of the Conjugate Gradient to the previously defined matrices.

\subsection*{4.}
Compute the logarithm energy norm of the error (i.e. $log((x - x^*)^T A(x - x^*))$)
for each matrix and plot it with respect to the number of iteration.\\

The same main file \textit{ex3.m} handles the visualization of the logarithm energy norm
of the error.
Figures %ref ref

\subsection*{5.}
Comment on the convergence of the method for the different matrices.
What can you say observing the number of iterations obtained
and the number of clusters of the eigenvalues of the related matrix?\\

The convergence of the method is directly related to the number of distinct eigenvalues
and their distribution in the matrix $A$.
Uniform distribution of the eigenvalues in the matrix $A$ leads to a balanced convergence
while a skewed distribution leads to a potentially slower or higher convergence,
dependent on the starting point and how many clusters of eigenvalues are there.
Figure %ref ref

\clearpage
% EXERCISE 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Exercise 4}
Consider the Chapter 4, "Trust-region methods" of the book \textit{Numerical Optimization},
Nocedal and Wright.

\subsection*{1.}
Explain  Cauchy point method and Dogleg method, as well as the connection between them.

\subsection*{2.}
Write down the Trust-Region algorithm, along with Dogleg and Cauchy-point computations.

\subsection*{3.}
Consider the following lemma (Lemma 4.2, page 75 Numerical optimization, Nocedal and Wright)

\mytheorem{lemma}{Lemma 4.2}{}{lemma:4.2}{
    Let B be positive definite. Then,
    \begin{itemize}
        \item[(a)] $|| \tilde{p}(\tau) ||$ is an increasing function of $\tau$, and
        \item[(b)] $m(\tilde{p}(\tau))$ is a decreasing function of $\tau$
    \end{itemize}
}

Read carefully the proof and explain in detail how each step is obtained.\\

\noindent
Recalling:

\myex{false}{eq:dogleg-path}{
    \tilde{p}(\tau) = \begin{cases}
        \tau p^U                                              & 0 \leq \tau \leq 1 \\
        p^U + \left( \tau - 1 \right) \left(p^B - p^U \right) & 1 \leq \tau \leq 2 \\
    \end{cases}
}

\noindent First, let's prove that $\norm{\tilde{p}(\tau)}$
is an increasing function with. $\tau \in [0, 1]$.

\begin{proof}
    Let's define $\phi(\tau) = \norm{\tilde{p}(\tau)}$.
    By computing $\frac{\partial \phi}{\partial \tau}$,
    since $\norm{p^U}$ is positive,
    the derivative is always positive and the function is monotonically increasing.

    \myex{false}{eq:ex4-3-1}{
        \phi(\tau) &= \norm{\tilde{p}(\tau)}
        = \abs{\tau} \cdot \norm{p^U}
        = \tau \norm{p^U} \\
        \frac{\partial \phi}{\partial \tau} &= \norm{p^U} \geq 0
    }

    thus, $\phi(\tau)$ is an increasing function with $\tau \in [0, 1]$.
\end{proof}

\noindent Second, let's prove that $m(\tilde{p}(\tau))$ is a decreasing function with
$\tau \in [0, 1]$.

\begin{proof}
    Let's expand the function $m(\tilde{p}(\tau))$.
    After computing its $\frac{\partial m}{\partial \tau}$,
    we expand $p^U$ by its definition and simplify the expression.
    Remember $B$ is SPD.
    At the end, since $\tau \in [0, 1]$, the expression $(\tau - 1) \leq 0$,
    thus the function is monotonically decreasing.

    \myex{false}{eq:ex4-3-2}{
        m(\tilde{p}(\tau))
        &= f + g^T \tilde{p}(\tau) + \frac{1}{2} \tilde{p}(\tau)^T B \tilde{p}(\tau)
        = f + \tau g^T p^U + \frac{1}{2} \tau^2 (p^U)^T B p^U \\
        \frac{\partial m}{\partial \tau} &= g^T p^U + \tau (p^U)^T B p^U \\
        &= g^T \cdot \left( - \frac{g^T g}{g^T B g} g \right)
        + \tau \left( - \frac{g^T g}{g^T B g} g \right)^T B \left( - \frac{g^T g}{g^T B g} g \right) \\
        &= g^T \cdot \left( - \frac{\norm{g}^2}{g^T B g} g \right)
        + \tau \left( - \frac{\norm{g}^2}{g^T B g} g \right)^T B \left( - \frac{\norm{g}^2}{g^T B g} g \right) \\
        &= - \frac{\norm{g}^4}{g^T B g} +\tau \frac{\norm{g}^4}{(g^T B g)^2} g^T B g
        = - \frac{\norm{g}^4}{g^T B g} +\tau \frac{\norm{g}^4}{g^T B g} \\
        &= \underbrace{\left( \tau - 1 \right)}_{\leq 0}
        \underbrace{\frac{\norm{g}^4}{g^T B g}}_{\geq 0} \leq 0 \\
    }

    thus, $m(\tilde{p}(\tau))$ is a decreasing function with $\tau \in [0, 1]$.
\end{proof}


\noindent Third, let's prove that $\norm{\tilde{p}(\tau)}$ is an increasing function with
$\tau \in [1, 2]$.

\begin{proof}{}
    Let's define $\alpha = \tau - 1$,
    and $\phi(\alpha) = \frac{1}{2} \norm{\tilde{p}(\alpha)}^2$ with $\alpha \in [0, 1]$.
    Inside $\phi(\alpha)$ we defined the squared norm $\norm{\cdot}^2$
    to delete the square root from the calculations.
    Then, simplify the expression by collecting using the square of sum which is the reason
    why we have the scalar $\frac{1}{2}$ in $\phi(\alpha)$.
    Remember $B$ is SPD.
    Afterwards, we compute $\frac{\partial \phi}{\partial \alpha}$
    and then prove $\left( p^U \right)^T \left( p^B - p^U \right) \geq 0$.
    To do so, we use the Cauchy-Schwarz inequality and exploit the $B$ matrix SPD properties,
    raising it to the square root to easy the computation of the norm of $u, v$ variables.
    Finally, this concludes the proof to show that $\phi$ is monotonically increasing.


    \myex{false}{eq:ex4-3-3}{
        \phi(\alpha) &= \frac{1}{2} \norm{\tilde{p}(\alpha)}^2
        = \frac{1}{2} \norm{p^U + \alpha \left( p^B - p^U \right)}^2 \\
        &= \frac{1}{2} \norm{p^U}^2 + \alpha \left( p^U \right)^T \left( p^B - p^U \right)
        + \frac{1}{2} \alpha^2 \norm{p^B - p^U}^2 \\
        \frac{\partial \phi}{\partial \alpha}
        &= \left( p^U \right)^T \left( p^B - p^U \right)
        + \underbrace{\alpha \norm{p^B - p^U}^2}_{\geq 0}
        \geq \left( p^U \right)^T \left( p^B - p^U \right) \\
        \left( p^U \right)^T \left( p^B - p^U \right)
        &= - \left( p^U \right)^T \left( p^U - p^B \right) \\
        &=  \frac{g^T g}{g^T B g} g^T
        \cdot \left[ \left( - \frac{g^T g}{g^T B g} g \right)
            - \left( - B^{-1} g \right) \right] \\
        &= \frac{\norm{g}^2}{g^T B g} g^T
        \cdot  \left( B^{-1} g
        - \frac{\norm{g}^2}{g^T B g} g \right) \\
        &= \frac{\norm{g}^2}{g^T B g} g^T B^{-1} g
        \cdot  \left( 1 - \frac{\norm{g}^2}{g^T B g B^{-1} g} g \right) \\
        &= \underbrace{\frac{\norm{g}^2}{g^T B g} g^T B^{-1} g}_{\geq 0}
        \cdot \left( 1 - \frac{\norm{g}^4}{
            \left( g^T B g \right) \left( g^T B^{-1} g \right)
        } \right) \\
        \text{let } u = B^{- \frac{1}{2}} g, v = B^{\frac{1}{2}} g
        &\Rightarrow \abs{u^T v}^2 \leq \norm{u}^2 \cdot \norm{v}^2 \\
        &\Rightarrow \abs{g^T B^{- \frac{1}{2}} B^{\frac{1}{2}} g}^2
        \leq \norm{B^{- \frac{1}{2}} g}^2 \cdot \norm{B^{\frac{1}{2}} g}^2 \\
        &\Rightarrow \abs{g^T g}^2
        \leq \left( g^T B^{- \frac{1}{2}} B^{- \frac{1}{2}} g \right)
        \left( g^T B^{\frac{1}{2}} B^{\frac{1}{2}} g \right) \\
        &\Rightarrow \norm{g}^4 \leq \left( g^T B^{-1} g \right) \left( g^T B g \right) \\
        &\Rightarrow \frac{\norm{g}^4}{
            \left( g^T B g \right) \left( g^T B^{-1} g \right)
        } \leq 1 \\
        &\Rightarrow 1 - \frac{\norm{g}^4}{
            \left( g^T B g \right) \left( g^T B^{-1} g \right)
        } \geq 0
    }

    thus, $\phi(\tau)$ is an increasing function with $\tau \in [1, 2]$.
\end{proof}

\noindent Lastly, let's prove that $m(\tilde{p}(\tau))$ is a decreasing function with
$\tau \in [1, 2]$.

\begin{proof}
    Let's define again $\alpha = \tau - 1$, for $m(\tilde{p}(\alpha))$
    with $\alpha \in [0, 1]$.

    \myex{false}{eq:ex4-3-4}{
    \phi(\alpha) &= m(\tilde{p}(\alpha))
    = f + g^T \tilde{p}(\alpha) + \frac{1}{2} \tilde{p}(\alpha)^T B \tilde{p}(\alpha) \\
    &= f + g^T \left[ p^U + \alpha \left( p^B - p^U \right) \right]
    + \frac{1}{2} \left[ p^U + \alpha \left( p^B - p^U \right) \right]^T B
    \left[ p^U + \alpha \left( p^B - p^U \right) \right] \\
    &= f + g^T p^U + \alpha g^T \left( p^B - p^U \right)
    + \frac{1}{2} \left( p^U + \alpha p^B - \alpha p^U \right) B
    \left( p^U + \alpha p^B - \alpha p^U \right) \\
    &= f + g^T p^U + \alpha g^T \left( p^B - p^U \right)
    + \frac{1}{2} [
        (p^U)^T B p^U + \alpha (p^U)^T B p^B - \alpha (p^U)^T B p^U + \alpha (p^B)^T B p^U \\
        &+ \alpha^2 (p^B)^T B p^B - \alpha^2 (p^B)^T B p^U - \alpha (p^U)^T B p^U
        - \alpha^2 (p^U)^T p^B + \alpha^2 (p^U)^T B p^U
    ] \\
    &= f + g^T p^U + \alpha g^T \left( p^B - p^U \right)
    + \frac{1}{2} (
    \alpha^2 \left[ (p^B)^T B p^B - (p^B)^T B p^U - (p^U)^T B p^B + (p^U)^T B p^U \right] \\
    &+ \alpha \left[ (p^U)^T B p^B - (p^U)^T B p^U + (p^B)^T B p^U - (p^U)^T B p^U \right]
    + (p^U)^T B p^U
    ) \\
    &= f + g^T p^U + \alpha g^T \left( p^B - p^U \right)
    +
    \frac{\alpha^2}{2} \left[ (p^B)^T B p^B + (p^U)^T B p^U - 2 (p^B)^T B p^U \right] \\
    &+ \alpha \left[ (p^U)^T B p^B - (p^U)^T B p^U \right]
    + (p^U)^T B p^U \\
    \frac{\partial \phi}{\partial \alpha}
    &= g^T \left( p^B - p^U \right)
    + \alpha \left[ (p^B)^T B p^B + (p^U)^T B p^U - 2 (p^B)^T B p^U \right]
    + \left[ (p^U)^T B p^B - (p^U)^T B p^U \right] \\
    &= g^T p^B - g^T p^U + \left[ (p^B)^T B p^U - (p^U)^T B p^U \right]
    + \alpha \left[ (p^B)^T B p^B - (p^U)^T B p^B + (p^U)^T B p^U - (p^B)^T B p^U \right] \\
    &= (p^B)^T g - (p^U)^T g + (p^B)^T B p^U - (p^U)^T B p^U
    + \alpha \left[ (p^B)^T B p^B - (p^U)^T B p^B + (p^U)^T B p^U - (p^B)^T B p^U \right] \\
    &= \left( p^B - p^U \right)^T \left(g + B p^U \right)
    + \alpha \left( p^B - p^U \right) B \left( p^B - p^U \right) \\
    \frac{\partial \phi}{\partial \alpha}
    &\leq \left( p^B - p^U \right)^T \left( g + B p^U + B \left( p^B - p^U \right) \right) \\
    &= \left( p^B - p^U \right)^T \left( g + B p^B \right) \\
    &= \left( p^B - p^U \right)^T \left[ g + B \left( - B^{-1} g \right) \right] =
    \left( p^B - p^U \right)^T \cdot \left( g - g \right) = 0
    }

    thus, $m(\tilde{p}(\tau))$ is a decreasing function with $\tau \in [1, 2]$.
\end{proof}

\end{document}
